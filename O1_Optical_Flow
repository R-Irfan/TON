import cv2
import numpy as np
import mediapipe as mp
from collections import OrderedDict

# Initialize MediaPipe Pose
mp_pose = mp.solutions.pose

# Function to get pose landmarks
def get_pose_landmarks(image, pose):
    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
    results = pose.process(image_rgb)
    if results.pose_landmarks:
        landmarks = results.pose_landmarks.landmark
        return landmarks
    else:
        return None

# Function to extract required keypoints
def extract_keypoints(landmarks, image_shape):
    keypoints = OrderedDict()
    h, w = image_shape[:2]
    mp_pose_landmark = mp.solutions.pose.PoseLandmark

    # Extracting keypoints and converting to pixel coordinates
    keypoints['left_shoulder'] = (landmarks[mp_pose_landmark.LEFT_SHOULDER].x * w,
                                  landmarks[mp_pose_landmark.LEFT_SHOULDER].y * h)
    keypoints['right_shoulder'] = (landmarks[mp_pose_landmark.RIGHT_SHOULDER].x * w,
                                   landmarks[mp_pose_landmark.RIGHT_SHOULDER].y * h)
    keypoints['left_elbow'] = (landmarks[mp_pose_landmark.LEFT_ELBOW].x * w,
                               landmarks[mp_pose_landmark.LEFT_ELBOW].y * h)
    keypoints['right_elbow'] = (landmarks[mp_pose_landmark.RIGHT_ELBOW].x * w,
                                landmarks[mp_pose_landmark.RIGHT_ELBOW].y * h)
    keypoints['left_wrist'] = (landmarks[mp_pose_landmark.LEFT_WRIST].x * w,
                               landmarks[mp_pose_landmark.LEFT_WRIST].y * h)
    keypoints['right_wrist'] = (landmarks[mp_pose_landmark.RIGHT_WRIST].x * w,
                                landmarks[mp_pose_landmark.RIGHT_WRIST].y * h)
    keypoints['left_hip'] = (landmarks[mp_pose_landmark.LEFT_HIP].x * w,
                             landmarks[mp_pose_landmark.LEFT_HIP].y * h)
    keypoints['right_hip'] = (landmarks[mp_pose_landmark.RIGHT_HIP].x * w,
                              landmarks[mp_pose_landmark.RIGHT_HIP].y * h)
    # Add more keypoints if needed
    return keypoints

# Load the t-shirt image (ideally with a person wearing it)
tshirt_img = cv2.imread('tshirt_image.jpg')

# Check if the image was loaded
if tshirt_img is None:
    print("Error: T-shirt image not found or failed to load.")
    exit()

# Extract keypoints from the t-shirt image
with mp_pose.Pose(static_image_mode=True,
                  model_complexity=2,
                  enable_segmentation=False,
                  min_detection_confidence=0.5) as pose:
    landmarks = get_pose_landmarks(tshirt_img, pose)
    if landmarks:
        tshirt_keypoints = extract_keypoints(landmarks, tshirt_img.shape)
    else:
        print("Error: No pose landmarks detected in the t-shirt image.")
        exit()

# Convert keypoints to NumPy array
src_points = np.array(list(tshirt_keypoints.values()), dtype=np.float32)

# Start the webcam feed
cap = cv2.VideoCapture(0)

# Variables for optical flow
prev_gray = None
prev_keypoints = None
smoothed_keypoints = None
alpha = 0.7  # Smoothing factor (0 < alpha <= 1)

with mp_pose.Pose(min_detection_confidence=0.5,
                  min_tracking_confidence=0.5) as pose:
    while cap.isOpened():
        success, frame = cap.read()
        if not success:
            print("Failed to capture image")
            break

        # Flip and convert the image to grayscale
        frame = cv2.flip(frame, 1)
        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)

        # Process the image and find pose landmarks
        results = pose.process(image)

        # Convert the image color back for rendering
        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)

        if results.pose_landmarks:
            # Extract landmark points
            landmarks = results.pose_landmarks.landmark

            # Extract keypoints from the landmarks
            dst_keypoints = extract_keypoints(landmarks, image.shape)

            # Convert destination points to NumPy array
            dst_points_array = np.array(list(dst_keypoints.values()), dtype=np.float32)

            # Prepare keypoints for optical flow
            current_keypoints = dst_points_array.copy().reshape(-1, 1, 2)

            if prev_gray is not None and prev_keypoints is not None:
                # Calculate optical flow
                new_keypoints, status, error = cv2.calcOpticalFlowPyrLK(
                    prev_gray, gray, prev_keypoints, None)

                # Update keypoints where flow was found
                for i, (new, old) in enumerate(zip(new_keypoints, prev_keypoints)):
                    if status[i]:
                        dst_points_array[i] = new.ravel()
                    else:
                        # If tracking failed, use pose estimation
                        dst_points_array[i] = current_keypoints[i].ravel()

            # Apply exponential moving average
            if smoothed_keypoints is None:
                smoothed_keypoints = dst_points_array.copy()
            else:
                smoothed_keypoints = alpha * dst_points_array + (1 - alpha) * smoothed_keypoints

            # Use smoothed keypoints for warping
            dst_points_array = smoothed_keypoints

            # Update previous frame and keypoints
            prev_gray = gray.copy()
            prev_keypoints = dst_points_array.copy().reshape(-1, 1, 2)

            # Ensure that the number of source and destination points match
            if src_points.shape[0] != dst_points_array.shape[0]:
                print("Error: Mismatch in number of keypoints.")
                continue

            # Get the list of keypoint names
            key_names = list(tshirt_keypoints.keys())
            index_left_shoulder = key_names.index('left_shoulder')
            index_right_shoulder = key_names.index('right_shoulder')

            # Compute shoulder distances in the t-shirt image
            left_shoulder_src = src_points[index_left_shoulder]
            right_shoulder_src = src_points[index_right_shoulder]
            src_shoulder_distance = np.linalg.norm(left_shoulder_src - right_shoulder_src)

            # Compute shoulder distances in the live image
            left_shoulder_dst = dst_points_array[index_left_shoulder]
            right_shoulder_dst = dst_points_array[index_right_shoulder]
            dst_shoulder_distance = np.linalg.norm(left_shoulder_dst - right_shoulder_dst)

            # Calculate the scaling factor
            scale_factor = dst_shoulder_distance / src_shoulder_distance

            # Resize the t-shirt image
            tshirt_img_resized = cv2.resize(tshirt_img, None, fx=scale_factor, fy=scale_factor, interpolation=cv2.INTER_LINEAR)

            # Adjust source keypoints
            src_points_scaled = src_points * scale_factor

            # Compute translation vector
            translation_vector = left_shoulder_dst - src_points_scaled[index_left_shoulder]

            # Apply translation
            src_points_translated = src_points_scaled + translation_vector

            # Create a bounding rectangle around the source points
            rect = cv2.boundingRect(src_points_translated)

            # Subdivide the bounding rectangle into triangles
            subdiv = cv2.Subdiv2D(rect)
            for p in src_points_translated:
                subdiv.insert((p[0], p[1]))

            # Get the list of triangles
            triangles = subdiv.getTriangleList()
            triangles = np.array(triangles, dtype=np.float32)

            # Function to find the index of a point
            def find_index(points, coord):
                for i, point in enumerate(points):
                    if np.linalg.norm(point - coord) < 1.0:
                        return i
                return -1

            # List to hold triangle indices
            triangle_indices = []

            for t in triangles:
                pts = [(t[0], t[1]), (t[2], t[3]), (t[4], t[5])]
                idx = []
                for pt in pts:
                    index = find_index(src_points_translated, np.array(pt))
                    if index != -1:
                        idx.append(index)
                if len(idx) == 3:
                    triangle_indices.append(idx)

            # Initialize output image
            output = image.copy()

            # Function to warp a triangle
            def warp_triangle(img1, img2, t1, t2):
                # Convert points to float32
                t1 = np.float32(t1)
                t2 = np.float32(t2)

                # Find bounding rectangles for each triangle
                r1 = cv2.boundingRect(t1)
                r2 = cv2.boundingRect(t2)

                x1, y1, w1, h1 = r1
                x2, y2, w2, h2 = r2

                # Ensure coordinates are within image boundaries
                x1 = max(0, min(x1, img1.shape[1] - 1))
                y1 = max(0, min(y1, img1.shape[0] - 1))
                x2 = max(0, min(x2, img2.shape[1] - 1))
                y2 = max(0, min(y2, img2.shape[0] - 1))

                # Recalculate widths and heights after clipping
                w1 = max(1, min(w1, img1.shape[1] - x1))
                h1 = max(1, min(h1, img1.shape[0] - y1))
                w2 = max(1, min(w2, img2.shape[1] - x2))
                h2 = max(1, min(h2, img2.shape[0] - y2))

                # Offset points by the top-left corner of the respective rectangles
                t1_rect = []
                t2_rect = []
                for i in range(3):
                    t1_rect.append((t1[i][0] - x1, t1[i][1] - y1))
                    t2_rect.append((t2[i][0] - x2, t2[i][1] - y2))

                # Create mask by filling the triangle
                mask = np.zeros((h2, w2), dtype=np.uint8)
                cv2.fillConvexPoly(mask, np.int32(t2_rect), 255)

                # Extract the patches from the images
                img1_rect = img1[y1:y1 + h1, x1:x1 + w1]

                # Check if img1_rect is valid
                if img1_rect.size == 0:
                    return

                # Warp the patch from img1 to img2
                size = (w2, h2)
                warp_mat = cv2.getAffineTransform(np.float32(t1_rect), np.float32(t2_rect))

                img2_rect = cv2.warpAffine(img1_rect, warp_mat, size, flags=cv2.INTER_LINEAR, borderMode=cv2.BORDER_REFLECT_101)

                # Ensure that img2_rect and mask have the same shape
                if img2_rect.shape[:2] != mask.shape:
                    img2_rect = cv2.resize(img2_rect, (mask.shape[1], mask.shape[0]))

                # Extract the region of interest from img2
                roi = img2[y2:y2 + h2, x2:x2 + w2]

                # Ensure roi, img2_rect, and mask have the same shape
                if roi.shape != img2_rect.shape:
                    img2_rect = cv2.resize(img2_rect, (roi.shape[1], roi.shape[0]))
                    mask = cv2.resize(mask, (roi.shape[1], roi.shape[0]))

                # Convert mask to boolean
                mask_bool = mask.astype(bool)

                # Blend the warped triangle into the destination image
                roi[mask_bool] = img2_rect[mask_bool]

                # Place the blended region back into the image
                img2[y2:y2 + h2, x2:x2 + w2] = roi

            # Warp each triangle
            for idx in triangle_indices:
                t1 = [src_points_translated[idx[0]], src_points_translated[idx[1]], src_points_translated[idx[2]]]
                t2 = [dst_points_array[idx[0]], dst_points_array[idx[1]], dst_points_array[idx[2]]]

                warp_triangle(tshirt_img_resized, output, t1, t2)

            # Display the final output
            cv2.imshow('Virtual Try-On', output)

        else:
            # No landmarks detected; display the frame
            cv2.imshow('Virtual Try-On', image)

        # Exit on pressing 'ESC'
        if cv2.waitKey(5) & 0xFF == 27:
            break

# Release resources
cap.release()
cv2.destroyAllWindows()
